# Configuration for Semantic Ego-Network GNN vs Baseline Comparison

data:
  path: "data/raw"

  temporal_windows:
    early_2018: "2018-Q1"
    mid_2018: "2018-Q2-Q3"
    late_2018: "2018-Q4"
    early_2019: "2019-Q1"

  depression_subreddits:
    - "depression"

  control_subreddits:
    - "conspiracy"
    - "divorce"
    - "fitness"
    - "guns"
    - "jokes"
    - "legaladvice"
    - "meditation"
    - "parenting"
    - "personalfinance"
    - "relationships"
    - "teaching"

  # Load 1000 posts per class (depression/control)
  # Most users have 1 post, so this yields ~1000 users per class after grouping
  target_posts_per_class: 1000

  # Sample sizes for training (CRITICAL for statistical power)
  # FAIR COMPARISON: Both models use SAME users with SAME splits
  max_users: 2000  # Target: 1000 depression + 1000 control (after stratified sampling)
  min_users_for_training: 100  # Minimum viable sample size

  # Shared train/val/test splits (applied BEFORE model training)
  test_size: 0.2    # 20% held out for final testing (400 users)
  val_size: 0.2     # 20% of remaining for validation (320 users from 1600)
                    # Results: 1280 train, 320 val, 400 test

  enable_chunked_loading: true
  chunk_size: 25000
  max_memory_mb: 16384

  enable_feature_cache: true
  cache_directory: "data/processed"

semantic_similarity:
  method: "sentence_transformers"
  model_name: "all-MiniLM-L6-v2"

  preprocessing:
    min_post_length: 10
    max_posts_per_user: 100
    remove_stopwords: false
    normalize_text: true

  temporal_aggregation: "mean"

ego_network:
  similarity_threshold: 0.6
  k_neighbors: 8
  max_neighbors: 50
  min_neighbors: 5
  k_hops: 2
  construction_method: "threshold"
  temporal_snapshots: false
  snapshot_intervals: ["early_2018", "mid_2018", "late_2018"]

models:
  baseline:
    type: "transformer"
    model_name: "roberta-base"
    max_length: 512
    hidden_dim: 256
    dropout: 0.3
    num_classes: 2

  semantic_gnn:
    type: "gat"
    input_dim: 135
    hidden_dim: 128
    output_dim: 64
    num_heads: 4
    num_layers: 3
    dropout: 0.4
    residual_connections: true

    temporal_component: true
    temporal_dim: 32

    feature_breakdown:
      semantic_dims: 64
      liwc_dims: 62
      temporal_dims: 9

training:
  epochs: 20
  batch_size: 32
  learning_rate: 0.001
  weight_decay: 0.0001

  patience: 5
  min_delta: 0.001

  optimizer: "adam"
  scheduler: "reduce_on_plateau"
  factor: 0.5
  scheduler_patience: 10

evaluation:
  metrics: ["accuracy", "precision", "recall", "f1", "auc", "specificity"]

  cv_folds: 5
  stratified: true

  early_detection:
    baseline_window: "early_2018"
    test_windows:
      - "mid_2018"
      - "late_2018"
      - "early_2019"

  comparison:
    baseline_vs_gnn: true
    statistical_tests: ["t_test", "wilcoxon"]
    confidence_level: 0.95

experiment:
  seed: 42
  log_level: "INFO"
  save_logs: true
  save_checkpoints: true
  checkpoint_dir: "checkpoints/"
  save_results: true
  results_dir: "results/"

ablation:
  similarity_methods: ["sentence_transformers", "bert", "tfidf"]
  thresholds: [0.4, 0.5, 0.6, 0.7, 0.8]
  max_neighbors: [20, 50, 100, 150]

  temporal_analysis:
    with_temporal: true
    without_temporal: true

  gnn_variants: ["gcn", "gat", "graphsage"]

interpretability:
  compute_feature_importance: true
  top_k_features: 20
  visualize_attention: true
  sample_size: 100
  network_statistics: true
  plot_networks: true

hardware:
  use_cuda: true
  device: "auto"
  batch_accumulation: 1
  gradient_clip: 1.0
